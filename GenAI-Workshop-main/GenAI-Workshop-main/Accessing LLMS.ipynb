{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbffd28c-9386-4862-ad84-3b2673d6865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad8f3238-48b0-45b1-b19c-c26e474f9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c1a298-edb6-4469-a9e9-69511cd41534",
   "metadata": {},
   "source": [
    "## Using SDK(Direct Library)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff489cf7-90f1-435c-9d25-9d80524771af",
   "metadata": {},
   "source": [
    "##### openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce13e29-0686-4fa7-8abe-9ca93a33b071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai   #Installing Open AI library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1edaae89-e4fd-4fe9-9c67-3fb40f5e2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c356cfc-4bf7-402c-9fb2-aff21a92760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(api_key =OPENAI_API_KEY )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c30de5a0-2d8d-4ac1-9f9f-88b35da41293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ENter the question what is Large Language Model \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model is a type of artificial intelligence model that is trained on a vast amount of text data in order to understand and generate human language. These models use deep learning techniques, particularly neural networks, to process and generate text. Large Language Models have the ability to understand context, syntax, and semantics of language, allowing them to generate coherent and contextually relevant text. They are used for a variety of natural language processing tasks such as text generation, translation, summarization, and more. Examples of Large Language Models include GPT-3 (Generative Pre-trained Transformer 3) developed by OpenAI and BERT (Bidirectional Encoder Representations from Transformers) developed by Google.\n"
     ]
    }
   ],
   "source": [
    "query = input(\"ENter the question\")\n",
    "response = llm.responses.create(model = 'gpt-3.5-turbo',temperature=0.5,input=query)\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3bd7dd-f5ba-443d-812d-3f1338d38938",
   "metadata": {},
   "source": [
    "### Gemini Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddf49d0a-2cbd-418f-a38c-3073da0347a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aba985a6-2ac6-40b9-9a8a-b794b684522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e50aecd-cfd9-4278-a5f1-5eadab35af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5e877bf-c599-4895-85e2-54633e948211",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key = GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a31e6ac9-0cdd-41fb-8d0c-50b72471bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(model_name = 'gemini-2.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9adb5f0e-0379-4316-b2be-08effbc96aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virat Kohli is an **Indian international cricketer**, widely regarded as **one of the greatest batsmen of all time** in the sport.\n",
      "\n",
      "Here's a breakdown of who he is:\n",
      "\n",
      "1.  **Nationality:** Indian\n",
      "2.  **Sport:** Cricket\n",
      "3.  **Role:** Right-handed top-order batsman\n",
      "4.  **Former Captain:** He captained the Indian national team in all three formats (Test, One Day International, and Twenty20 International) for a significant period, leading them to historic victories and No. 1 rankings.\n",
      "5.  **Batting Style:** Known for his aggressive yet classical style of play, incredible consistency, exceptional ability to perform under pressure, and mastery of run-chases.\n",
      "6.  **Records and Achievements:**\n",
      "    *   He holds numerous batting records across all formats of the game.\n",
      "    *   He is the player with the most centuries in One Day Internationals (ODI), surpassing Sachin Tendulkar.\n",
      "    *   He has the most runs in T20 Internationals.\n",
      "    *   He was part of the Indian team that won the 2011 Cricket World Cup.\n",
      "    *   He is one of the few batsmen to average over 50 in all three international formats for a long duration of his career.\n",
      "7.  **Fitness Icon:** Kohli is also renowned for his extreme fitness regimen, which has set new benchmarks for professional cricketers globally.\n",
      "8.  **Personality:** He is known for his passionate and aggressive demeanor on the field, often called \"King Kohli\" by fans and media.\n",
      "\n",
      "In essence, Virat Kohli is a modern legend of cricket, celebrated for his unparalleled batting prowess, leadership, fitness, and immense impact on the game globally.\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"Who is virat kohli?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093ddb32-ce3a-4204-b9cd-34f459547920",
   "metadata": {},
   "source": [
    "# Acesssing LLMS using LANGCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97bf78e5-5c8b-4c37-813a-37cb81f08dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9994cdb1-c6f3-4bf9-83e2-bc2de2d84af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI,ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5022eefb-ee34-4427-8226-adc157f6ecf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for \"Large Language Model.\" It refers to a type of artificial intelligence model that is designed to understand, generate, and manipulate human language. These models are typically built using deep learning techniques, particularly neural networks, and are trained on vast amounts of text data to learn the statistical patterns and structures of language.\n",
      "\n",
      "### Key Features of LLMs:\n",
      "\n",
      "1. **Scale**: LLMs are characterized by their large number of parameters, often in the billions or even trillions. This scale allows them to capture complex language patterns and nuances.\n",
      "\n",
      "2. **Training Data**: They are trained on diverse datasets that include books, articles, websites, and other text sources. This broad exposure helps them understand various topics and styles of writing.\n",
      "\n",
      "3. **Natural Language Processing (NLP)**: LLMs are used for a variety of NLP tasks, including text generation, translation, summarization, sentiment analysis, and question-answering.\n",
      "\n",
      "4. **Contextual Understanding**: They can generate contextually relevant responses by considering the surrounding text, which allows for more coherent and context-aware interactions.\n",
      "\n",
      "5. **Transfer Learning**: Many LLMs are pre-trained on a large corpus of text and then fine-tuned on specific tasks, allowing them to adapt to various applications with relatively little additional training.\n",
      "\n",
      "### Applications of LLMs:\n",
      "\n",
      "- **Chatbots and Virtual Assistants**: Providing conversational agents that can interact with users in a natural manner.\n",
      "- **Content Creation**: Assisting in writing articles, stories, or even code.\n",
      "- **Language Translation**: Offering real-time translation services between different languages.\n",
      "- **Information Retrieval**: Enhancing search engines and knowledge bases by providing more accurate and relevant answers to queries.\n",
      "\n",
      "### Challenges and Considerations:\n",
      "\n",
      "- **Bias**: LLMs can inadvertently learn and propagate biases present in the training data, leading to ethical concerns.\n",
      "- **Misinformation**: They can generate plausible-sounding but factually incorrect information.\n",
      "- **Resource Intensive**: Training and deploying LLMs require significant computational resources and energy.\n",
      "\n",
      "Overall, LLMs represent a significant advancement in the field of artificial intelligence and natural language processing, enabling a wide range of applications that enhance human-computer interaction.\n"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(model = 'gpt-4o-mini',api_key=OPENAI_API_KEY,temperature=0.2)\n",
    "response = model.invoke(\"Explain LLM\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0227c53-dd22-408e-99a0-4306d63710f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d2e3cfb-21bd-4cb6-beac-7d430559187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI,ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04c0b33e-3694-4be7-8453-f6bbfcab463c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**K-Nearest Neighbors (KNN)** is a simple, non-parametric, lazy learning algorithm used for both **classification** and **regression** tasks. It's one of the most fundamental algorithms in machine learning.\n",
      "\n",
      "The core idea behind KNN is intuitive: **\"Birds of a feather flock together\"** or **\"You are who your neighbors are.\"** When you want to classify a new data point, KNN looks at its 'K' closest data points (neighbors) in the training set and assigns the new point a class based on the majority class among those neighbors.\n",
      "\n",
      "---\n",
      "\n",
      "### How KNN Works (Step-by-Step):\n",
      "\n",
      "Let's imagine you have a dataset of fruits (apples and oranges) based on their color and size, and you want to classify a new, unknown fruit.\n",
      "\n",
      "1.  **Choose the number of neighbors (K):** This is the most crucial hyperparameter. You decide how many \"neighbors\" to consider. For example, if K=3, you'll look at the 3 closest fruits.\n",
      "\n",
      "2.  **Calculate Distances:** For the new, unknown data point, KNN calculates its distance to *every single* data point in the training set. Common distance metrics include:\n",
      "    *   **Euclidean Distance:** The straight-line distance between two points (most common).\n",
      "    *   **Manhattan Distance:** The sum of the absolute differences of their Cartesian coordinates (like navigating a city grid).\n",
      "    *   **Minkowski Distance:** A generalization of both Euclidean and Manhattan distances.\n",
      "    *   **Cosine Similarity:** Often used for high-dimensional data like text, measuring the angle between two vectors.\n",
      "\n",
      "3.  **Identify K-Nearest Neighbors:** After calculating all distances, KNN sorts them in ascending order and selects the 'K' data points that have the smallest distances to the new point. These are your K-nearest neighbors.\n",
      "\n",
      "4.  **Predict the Output:**\n",
      "    *   **For Classification:** The new data point is assigned the class that is most frequent among its K-nearest neighbors (a majority vote). If K=3 and two neighbors are \"apple\" and one is \"orange,\" the new fruit is classified as \"apple.\"\n",
      "    *   **For Regression:** The new data point's value is typically the average (or median) of the values of its K-nearest neighbors.\n",
      "\n",
      "---\n",
      "\n",
      "### Key Characteristics of KNN:\n",
      "\n",
      "*   **Non-parametric:** KNN makes no assumptions about the underlying distribution of the data. This makes it flexible for complex datasets.\n",
      "*   **Lazy Learner:** KNN is called a \"lazy learner\" because it does not build a model during a \"training phase.\" Instead, all computation is deferred until a prediction is requested. It simply stores the entire training dataset.\n",
      "*   **Instance-based:** It relies directly on the instances (data points) in the training set to make predictions.\n",
      "\n",
      "---\n",
      "\n",
      "### Choosing the Value of K:\n",
      "\n",
      "*   **Small K (e.g., K=1):** Makes the model highly sensitive to noise and outliers. The decision boundary will be very complex and jagged.\n",
      "*   **Large K:** Provides a smoother decision boundary, reduces the effect of noise, but can blur the distinctions between classes and make the model computationally more expensive.\n",
      "*   **General Practice:** K is often chosen as an odd number to avoid ties in binary classification. The optimal K is usually found through techniques like cross-validation.\n",
      "\n",
      "---\n",
      "\n",
      "### Advantages of KNN:\n",
      "\n",
      "*   **Simple to understand and implement:** Its logic is straightforward.\n",
      "*   **No training phase:** As a lazy learner, there's no explicit model building, which can be an advantage when data is constantly changing.\n",
      "*   **Non-parametric:** It's effective for non-linear data and doesn't require assumptions about data distribution.\n",
      "*   **Handles multi-class problems naturally:** It can easily be extended to problems with more than two classes.\n",
      "\n",
      "---\n",
      "\n",
      "### Disadvantages of KNN:\n",
      "\n",
      "*   **Computationally expensive at prediction time:** For every new data point, it needs to calculate distances to *all* training points, which can be very slow for large datasets.\n",
      "*   **Memory intensive:** It needs to store the entire training dataset.\n",
      "*   **Sensitive to the curse of dimensionality:** In high-dimensional spaces, the concept of \"distance\" becomes less meaningful, and KNN's performance can degrade significantly.\n",
      "*   **Sensitive to feature scaling:** Features with larger ranges will dominate the distance calculations. **Feature scaling (normalization or standardization) is crucial for KNN.**\n",
      "*   **Sensitive to noisy data and outliers:** Especially with a small K, outliers can heavily influence the classification.\n",
      "*   **Imbalanced data:** If one class is much more frequent than others, KNN might be biased towards the majority class.\n",
      "\n",
      "---\n",
      "\n",
      "### Pre-processing Steps for KNN:\n",
      "\n",
      "Due to its sensitivity, KNN often benefits greatly from:\n",
      "\n",
      "1.  **Feature Scaling:** Normalize or standardize your features (e.g., using Min-Max Scaling or Z-score Standardization).\n",
      "2.  **Dimensionality Reduction:** If you have many features, techniques like PCA (Principal Component Analysis) can help reduce the number of dimensions and improve performance.\n",
      "3.  **Outlier Handling:** Identify and treat outliers in your data.\n",
      "\n",
      "---\n",
      "\n",
      "### Use Cases:\n",
      "\n",
      "*   **Recommendation Systems:** Suggesting products or content based on what similar users have liked.\n",
      "*   **Image Recognition:** Simple image classification tasks.\n",
      "*   **Credit Scoring:** Assessing the creditworthiness of loan applicants.\n",
      "*   **Medical Diagnosis:** Identifying diseases based on patient symptoms.\n",
      "*   **Pattern Recognition:** General classification tasks in various domains.\n",
      "\n",
      "In summary, KNN is a powerful and intuitive algorithm, especially useful when the decision boundary is complex and not easily defined by a simple parametric model. However, its computational cost and sensitivity to data characteristics mean that proper pre-processing and careful selection of K are essential for good performance.\n"
     ]
    }
   ],
   "source": [
    "model = ChatGoogleGenerativeAI(model='gemini-2.5-flash',api_key=GEMINI_API_KEY,temperature=0.4)\n",
    "response = model.invoke(\"Explain about KNN\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d271eed5-9eab-484c-ab16-0d13e7eaa41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs, or Large Language Models, are a type of artificial intelligence designed to understand, generate, and manipulate human language. They are built using deep learning techniques, particularly neural networks, and are trained on vast amounts of text data from diverse sources, such as books, articles, websites, and more. Here are some key aspects of LLMs:\n",
      "\n",
      "### 1. **Architecture**\n",
      "   - **Neural Networks**: LLMs typically use architectures like Transformers, which are particularly effective for processing sequential data, such as text. The Transformer architecture allows the model to consider the context of words in relation to each other, rather than just in a linear fashion.\n",
      "   - **Layers and Parameters**: LLMs consist of multiple layers of neurons, and they can have billions or even trillions of parameters. These parameters are adjusted during training to minimize the difference between the model's predictions and the actual outcomes.\n",
      "\n",
      "### 2. **Training Process**\n",
      "   - **Pre-training**: LLMs are initially trained on a large corpus of text in an unsupervised manner. During this phase, the model learns to predict the next word in a sentence given the previous words (language modeling) or to fill in missing words (masked language modeling).\n",
      "   - **Fine-tuning**: After pre-training, LLMs can be fine-tuned on specific tasks or datasets, such as sentiment analysis, translation, or question answering. This phase often involves supervised learning, where the model learns from labeled examples.\n",
      "\n",
      "### 3. **Capabilities**\n",
      "   - **Text Generation**: LLMs can generate coherent and contextually relevant text based on a given prompt. This can be used for creative writing, content generation, and more.\n",
      "   - **Understanding Context**: They can understand and respond to questions, summarize texts, translate languages, and perform various other language-related tasks.\n",
      "   - **Conversational Agents**: LLMs can be used to create chatbots and virtual assistants that can engage in human-like conversations.\n",
      "\n",
      "### 4. **Applications**\n",
      "   - **Customer Support**: Automating responses to common inquiries.\n",
      "   - **Content Creation**: Assisting writers by generating ideas or drafting text.\n",
      "   - **Education**: Providing tutoring or personalized learning experiences.\n",
      "   - **Research**: Summarizing articles or extracting relevant information from large datasets.\n",
      "\n",
      "### 5. **Challenges and Limitations**\n",
      "   - **Bias**: LLMs can inadvertently learn and propagate biases present in the training data, leading to biased or inappropriate outputs.\n",
      "   - **Misinformation**: They can generate plausible-sounding but factually incorrect information.\n",
      "   - **Resource Intensive**: Training and deploying LLMs require significant computational resources and energy, raising concerns about sustainability.\n",
      "\n",
      "### 6. **Ethical Considerations**\n",
      "   - The use of LLMs raises ethical questions regarding privacy, misinformation, and the potential for misuse in generating harmful content. Responsible deployment and ongoing research into mitigating these issues are crucial.\n",
      "\n",
      "In summary, LLMs represent a significant advancement in natural language processing, enabling a wide range of applications while also presenting challenges that need to be addressed.\n"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(model = 'gpt-4o-mini',api_key=OPENAI_API_KEY,temperature=0)\n",
    "response = model.invoke(\"Explain LLMs\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "450c95ba-6a2e-4bb8-a1ec-71f431b88866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLMs, or Large Language Models, are a'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI(model = 'gpt-4o-mini',api_key=OPENAI_API_KEY,temperature=1,max_completion_tokens=10)\n",
    "response = model.invoke(\"Explain LLMs\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92cd9b17-1776-42dd-a44e-d675853588b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI,ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db5a6a9c-7bd0-456a-a830-716c8f07cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = 'gpt-3.5-turbo-instruct',api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e2ed371-44bc-4cbf-a79d-d07ea5c44567",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExplain LLM\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GENAI\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GENAI\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1123\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1114\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1116\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1120\u001b[39m     **kwargs: Any,\n\u001b[32m   1121\u001b[39m ) -> LLMResult:\n\u001b[32m   1122\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GENAI\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:933\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    932\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    939\u001b[39m         )\n\u001b[32m    940\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    941\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GENAI\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1235\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1233\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1234\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1235\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1239\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GENAI\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1422\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1420\u001b[39m     _handle_openai_bad_request(e)\n\u001b[32m   1421\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.APIError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m     \u001b[43m_handle_openai_api_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1423\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GENAI\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1417\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1410\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1411\u001b[39m             response,\n\u001b[32m   1412\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1413\u001b[39m             metadata=generation_info,\n\u001b[32m   1414\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1415\u001b[39m         )\n\u001b[32m   1416\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1417\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1418\u001b[39m         response = raw_response.parse()\n\u001b[32m   1419\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GENAI\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GENAI\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GENAI\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GENAI\\Lib\\site-packages\\openai\\_base_client.py:1297\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1288\u001b[39m     warnings.warn(\n\u001b[32m   1289\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1290\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1291\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1292\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1293\u001b[39m     )\n\u001b[32m   1294\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1295\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, content=content, files=to_httpx_files(files), **options\n\u001b[32m   1296\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\GENAI\\Lib\\site-packages\\openai\\_base_client.py:1070\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1067\u001b[39m             err.response.read()\n\u001b[32m   1069\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1072\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'message': 'This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}"
     ]
    }
   ],
   "source": [
    "llm.invoke(\"Explain LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2f3a8afc-b430-42e8-9fe8-38bc1d0c3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model='gpt-3.5-turbo-instruct',api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc38277f-dc2a-4ade-aca1-4092d9088c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NR\\n\\nLLMNR (Link-Local Multicast Name Resolution) is a protocol used for name resolution in local networks. It is a Microsoft proprietary protocol that allows computers to resolve hostnames to IP addresses without the use of a DNS server. This is particularly useful in networks that do not have a DNS server, such as small home networks.\\n\\nLLMNR uses multicast to send name resolution queries to all devices on the local network, and the device with the matching hostname responds with its IP address. This allows devices to communicate with each other without the need for a central DNS server.\\n\\nLLMNR operates on IPv4 and IPv6, and is enabled by default on Windows operating systems. It is also supported by some network devices, such as routers and switches.\\n\\nOne potential security concern with LLMNR is the possibility of man-in-the-middle attacks, where an attacker can intercept and manipulate the name resolution process to redirect traffic to malicious websites. To prevent this, it is recommended to disable LLMNR and use a DNS server instead for name resolution.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Explain LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3926be7b-7a1d-4d0b-892a-c7752cdba9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
