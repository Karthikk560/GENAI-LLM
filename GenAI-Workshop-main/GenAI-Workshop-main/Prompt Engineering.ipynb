{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf454758-3004-4404-975c-b3f013881cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "518b95b4-ec43-42a0-a0c7-47893ee27218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400e369b-b84f-4dad-9324-a34868033e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model='gemini-2.5-flash',api_key=GEMINI_API_KEY,temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad7799-a9ea-4c94-a0dc-6e5ee2bb1305",
   "metadata": {},
   "source": [
    "### ZERO SHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad72ba45-21f2-4f0e-9be1-4e4302a4c9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you're studying for a test.\n",
      "\n",
      "**The Simple Analogy:**\n",
      "\n",
      "1.  **You get a practice test.** You study it so hard that you not only learn the answers to the questions, but you also memorize the *exact wording* of each question, the specific font used, and even the little coffee stain on the corner of one page.\n",
      "2.  **You take the practice test again.** You get a perfect score! You know everything about *that specific test*.\n",
      "3.  **Now, you take the actual test.** The questions cover the same *general topics*, but they are worded differently, in a different font, and there are no coffee stains.\n",
      "4.  **The problem:** Because you memorized the practice test *too specifically* (including all the irrelevant details like font and coffee stains), you struggle with the actual test. You can't apply your knowledge to slightly different situations. You might even fail, even though you \"knew\" the practice test perfectly.\n",
      "\n",
      "---\n",
      "\n",
      "**In Machine Learning, \"Overfit\" is exactly like this:**\n",
      "\n",
      "*   **The \"Student\" is your Machine Learning Model.**\n",
      "*   **The \"Practice Test\" is your Training Data.** This is the data you show the model so it can learn patterns.\n",
      "*   **The \"Actual Test\" is New, Unseen Data (or Test Data).** This is data the model has never seen before, which it needs to make predictions on.\n",
      "\n",
      "**What Overfit Means:**\n",
      "\n",
      "An overfit model has learned the training data **too perfectly**. It doesn't just learn the general, useful patterns; it also memorizes the specific quirks, random errors, or \"noise\" that are unique to *that particular training dataset*.\n",
      "\n",
      "**The Consequence:**\n",
      "\n",
      "*   **On the training data:** The overfit model performs **amazingly well**. It gets almost everything right because it has essentially memorized it.\n",
      "*   **On new, unseen data:** The overfit model performs **very poorly**. It tries to apply its overly specific \"memorized\" rules, which don't quite fit the new situations, leading to inaccurate predictions.\n",
      "\n",
      "**In short:** An overfit model is great at explaining the past (the data it trained on), but terrible at predicting the future (new data). It has failed to learn the *generalizable* rules.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template('Explain the concept of {topic} in simple terms.')\n",
    "chain = prompt | model | StrOutputParser()\n",
    "print(chain.invoke({'topic':'Overfit'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1647df-197c-4dce-b3ea-e4639657763c",
   "metadata": {},
   "source": [
    "## FEW SHOT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cde5e58e-a193-446f-b4f9-c7926e85a4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Classify the sentiment of the given text as Positive, Negative, or Neutral.\n",
    "\n",
    "Text: I loved the product!\n",
    "Sentiment: Positive\n",
    "\n",
    "Text: The service was horrible.\n",
    "Sentiment: Negative\n",
    "\n",
    "Text: It was okay, nothing special.\n",
    "Sentiment: Neutral\n",
    "\n",
    "Text: {query}\n",
    "Sentiment:\"\"\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "print(chain.invoke({'query':'The Service was great'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba49b2e7-e0eb-4448-a80a-5dd884efecf9",
   "metadata": {},
   "source": [
    "## Instruction Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6e85815-e6e7-4d80-8da9-956c46020c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN (K-Nearest Neighbors) is a non-parametric, lazy learning algorithm used for both classification and regression. It doesn't build a model during training; instead, it memorizes the entire training dataset.\n",
      "\n",
      "**Example:** To classify a new data point, KNN finds the 'K' closest data points (neighbors) in the training set. If K=3, it identifies the 3 nearest existing points. The new point is then assigned the class most common among these K neighbors (e.g., if 2 of 3 neighbors are \"Cat\" and 1 is \"Dog\", it's classified as \"Cat\"). For regression, it would average their values.\n",
      "\n",
      "**Key Formula:** The \"closeness\" is typically determined by distance metrics. The most common is Euclidean distance between two points $p$ and $q$:\n",
      "\n",
      "$d(p,q) = \\sqrt{\\sum_{i=1}^{n}(q_i - p_i)^2}$\n",
      "\n",
      "Here, $n$ is the number of features, and $p_i, q_i$ are the values of the $i$-th feature for points $p$ and $q$.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Explain {topic} using:\n",
    "1. Definition\n",
    "2. Example\n",
    "3. Key formula\n",
    "Limit to 150 words.\n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "print(chain.invoke({'topic':'KNN'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f10e04-ef28-4646-b55a-fe2d39edecb2",
   "metadata": {},
   "source": [
    "## Role Based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0fc28b2-f3bc-4827-9ce7-ee5ad71c0da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, let's dive deep into Random Forests. As a senior ML engineer, I've seen this algorithm save countless projects due to its robustness and predictive power. It's a cornerstone ensemble method, elegantly combining simplicity with sophisticated statistical properties.\n",
      "\n",
      "### Random Forest: An Ensemble of Decorrelated Decision Trees\n",
      "\n",
      "At its core, a Random Forest is an **ensemble learning method** for classification and regression that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
      "\n",
      "The \"magic\" of Random Forests lies in two key concepts that address the inherent limitations of single decision trees:\n",
      "\n",
      "1.  **High Variance of Decision Trees:** Individual decision trees, especially deep ones, are prone to overfitting. They can learn highly complex, specific patterns from the training data, leading to poor generalization on unseen data.\n",
      "2.  **Correlation Between Trees:** If we simply trained many decision trees on the *same* data, they would likely make similar errors, and averaging them wouldn't significantly reduce the overall error. We need diversity.\n",
      "\n",
      "Random Forest tackles these issues through two forms of \"randomness\": **Bagging (Bootstrap Aggregating)** and **Feature Randomness**.\n",
      "\n",
      "---\n",
      "\n",
      "### The \"Random\" Part: Sources of Diversity\n",
      "\n",
      "#### 1. Bagging (Bootstrap Aggregating)\n",
      "\n",
      "This is the foundational ensemble technique for Random Forests, introduced by Leo Breiman.\n",
      "\n",
      "*   **Mechanism:** Instead of training all trees on the original training dataset $D = \\{(x_i, y_i)\\}_{i=1}^N$, Bagging creates $K$ new training datasets $D_k$ by **sampling with replacement** from $D$. Each $D_k$ will have the same size $N$ as the original dataset, but due to sampling with replacement, some original samples will appear multiple times, while others (approximately 37%) will not appear at all (these are called Out-Of-Bag, or OOB, samples, which we'll discuss later).\n",
      "*   **Intuition (Mathematical): Variance Reduction**\n",
      "    Imagine you have $K$ independent random variables $Z_1, \\dots, Z_K$, each with mean $\\mu$ and variance $\\sigma^2$. The variance of their average is $\\text{Var}\\left(\\frac{1}{K}\\sum_{k=1}^K Z_k\\right) = \\frac{1}{K^2}\\sum_{k=1}^K \\text{Var}(Z_k) = \\frac{1}{K^2} K\\sigma^2 = \\frac{\\sigma^2}{K}$.\n",
      "    This shows that averaging independent estimates dramatically reduces variance.\n",
      "    In the context of Random Forests, each decision tree $h_k(x)$ trained on a bootstrap sample $D_k$ can be thought of as an estimator. While these estimators are not perfectly independent (they are trained on samples from the same underlying distribution), bootstrapping introduces enough diversity to significantly reduce the variance of the ensemble prediction. The overall model $H(x) = \\frac{1}{K}\\sum_{k=1}^K h_k(x)$ (for regression) or $H(x) = \\text{mode}(\\{h_k(x)\\}_{k=1}^K)$ (for classification) will have much lower variance than any single $h_k(x)$.\n",
      "\n",
      "#### 2. Feature Randomness (Random Subspace Method)\n",
      "\n",
      "This is the critical addition that distinguishes Random Forests from plain Bagging of decision trees.\n",
      "\n",
      "*   **Mechanism:** When growing each decision tree in the forest, at *each split point* (node), instead of considering all $M$ available features to find the best split, a random subset of $m \\ll M$ features is chosen. The algorithm then searches for the best split *only within these $m$ features*. A typical value for $m$ is $\\sqrt{M}$ for classification and $M/3$ for regression.\n",
      "*   **Intuition (Mathematical): Decorrelation of Trees**\n",
      "    Consider a dataset where one or a few features are very strong predictors. If we only used Bagging, most trees would likely pick these strong features near the root, leading to highly similar (and thus highly correlated) trees.\n",
      "    If we have $K$ trees, $h_1, \\dots, h_K$, the variance of their average prediction is given by:\n",
      "    $\\text{Var}\\left(\\frac{1}{K}\\sum_{k=1}^K h_k(x)\\right) = \\frac{1}{K}\\text{Var}(h_k(x)) + \\frac{K-1}{K}\\text{Cov}(h_i(x), h_j(x))$\n",
      "    where $\\text{Var}(h_k(x))$ is the average variance of a single tree, and $\\text{Cov}(h_i(x), h_j(x))$ is the average pairwise covariance between trees.\n",
      "    The feature randomness directly addresses the $\\text{Cov}(h_i(x), h_j(x))$ term. By forcing trees to consider different subsets of features at each split, it makes them more diverse and less correlated. Even if a strong feature is present, not all trees will be allowed to use it at the top splits, leading to trees that explore different predictive pathways. This reduction in covariance is crucial for the overall variance reduction of the ensemble.\n",
      "\n",
      "---\n",
      "\n",
      "### The \"Forest\" Part: Deep Decision Trees\n",
      "\n",
      "Each individual tree in a Random Forest is typically grown to its maximum depth (or near maximum depth) without pruning.\n",
      "\n",
      "*   **Why unpruned?** A single, unpruned decision tree is a high-variance, low-bias model. It tends to overfit its training data (the specific bootstrap sample). However, the power of the ensemble comes from averaging many such high-variance, low-bias models. The Bagging and feature randomness mechanisms ensure that these individual high-variance models are sufficiently diverse, so their individual overfitting tendencies cancel each other out when aggregated. The ensemble effectively reduces the variance without significantly increasing the bias (as individual trees are allowed to learn complex patterns).\n",
      "\n",
      "---\n",
      "\n",
      "### Algorithm Steps\n",
      "\n",
      "1.  For $k = 1$ to `n_estimators` (number of trees):\n",
      "    a.  **Bootstrap Sampling:** Draw a bootstrap sample $D_k$ of size $N$ from the original training data $D$.\n",
      "    b.  **Tree Construction:** Grow a decision tree $h_k$ from $D_k$.\n",
      "        i.  At each node of the tree, randomly select $m$ features from the total $M$ features.\n",
      "        ii. Find the best split among these $m$ features (e.g., using Gini impurity for classification or mean squared error for regression).\n",
      "        iii. Split the node into two daughter nodes.\n",
      "        iv. Repeat steps i-iii until a stopping criterion is met (e.g., maximum depth, minimum samples per leaf, or perfect classification/pure node). Typically, trees are grown deep, often until leaf nodes contain a minimum number of samples or are pure.\n",
      "2.  **Prediction:**\n",
      "    *   **Classification:** For a new instance $x$, each tree $h_k(x)$ predicts a class. The final prediction $H(x)$ is the majority vote among all $K$ trees.\n",
      "    *   **Regression:** For a new instance $x$, each tree $h_k(x)$ predicts a numerical value. The final prediction $H(x)$ is the average of the predictions from all $K$ trees.\n",
      "\n",
      "---\n",
      "\n",
      "### Mathematical Intuition: Bias-Variance Trade-off\n",
      "\n",
      "*   **Single Deep Tree:** Low bias (can model complex relationships), High variance (overfits to specific training data).\n",
      "*   **Random Forest:**\n",
      "    *   **Bias:** Remains relatively low because individual trees are deep and allowed to learn complex patterns. The ensemble doesn't significantly constrain the model's capacity to fit the underlying function.\n",
      "    *   **Variance:** Significantly reduced by averaging/voting across many decorrelated trees. The random sampling and feature selection ensure that the errors of individual trees are not perfectly correlated, allowing them to cancel out in the aggregation step.\n",
      "\n",
      "This makes Random Forests a powerful \"off-the-shelf\" algorithm that often performs very well without extensive hyperparameter tuning.\n",
      "\n",
      "---\n",
      "\n",
      "### Key Hyperparameters\n",
      "\n",
      "1.  **`n_estimators`**: The number of trees in the forest.\n",
      "    *   **Impact:** More trees generally lead to better performance and more stable predictions, but with diminishing returns and increased computational cost. You typically want enough trees for the OOB error to stabilize.\n",
      "2.  **`max_features`**: The number of features to consider when looking for the best split.\n",
      "    *   **Impact:** This is crucial for controlling the bias-variance trade-off and decorrelating trees.\n",
      "        *   Smaller `max_features`: Increases tree diversity (lower correlation), potentially increasing bias of individual trees (as they have fewer options).\n",
      "        *   Larger `max_features`: Decreases tree diversity (higher correlation), potentially decreasing bias of individual trees.\n",
      "    *   **Common values:** $\\sqrt{\\text{num_features}}$ for classification, $\\text{num_features}/3$ for regression.\n",
      "3.  **`max_depth`**: The maximum depth of the tree.\n",
      "    *   **Impact:** Often left `None` (trees grow until leaves are pure or contain `min_samples_leaf` samples) to leverage the ensemble's ability to handle overfitting. Constraining `max_depth` can increase bias.\n",
      "4.  **`min_samples_leaf`**: The minimum number of samples required to be at a leaf node.\n",
      "    *   **Impact:** Helps control the granularity of the tree. Larger values prevent trees from learning overly specific patterns (pruning effect).\n",
      "5.  **`bootstrap`**: Whether bootstrap samples are used when building trees.\n",
      "    *   **Impact:** Almost always `True`. Setting to `False` means each tree is trained on the full dataset, effectively turning it into a simple Bagging ensemble without the added variance reduction from bootstrap sampling.\n",
      "\n",
      "---\n",
      "\n",
      "### Advantages\n",
      "\n",
      "*   **Robustness to Overfitting:** Thanks to Bagging and feature randomness.\n",
      "*   **Handles High Dimensionality:** Can work with a large number of features.\n",
      "*   **Handles Non-linearity:** Decision trees are inherently non-linear models.\n",
      "*   **Implicit Feature Selection:** The algorithm can provide estimates of feature importance (e.g., Gini importance or permutation importance).\n",
      "*   **Handles Missing Values:** Can be adapted to handle missing values (e.g., by imputing or using surrogate splits).\n",
      "*   **Out-of-Bag (OOB) Error Estimation:** Since each tree is trained on a bootstrap sample, approximately 37% of the original data is \"out-of-bag\" for that tree. These OOB samples can be used as a validation set for that specific tree, and the average OOB error across all trees provides a robust, unbiased estimate of the generalization error without needing a separate validation set.\n",
      "\n",
      "---\n",
      "\n",
      "### Disadvantages\n",
      "\n",
      "*   **Interpretability:** Less interpretable than a single decision tree due to the ensemble nature. It's a \"black box\" to some extent.\n",
      "*   **Computational Cost:** Can be computationally expensive and memory-intensive for large datasets and many trees.\n",
      "*   **Prediction Speed:** Slower than a single decision tree for predictions, as all trees need to be evaluated.\n",
      "\n",
      "---\n",
      "\n",
      "### Practical Considerations\n",
      "\n",
      "Random Forests are a go-to algorithm for many tabular data problems. They are often a strong baseline and can achieve state-of-the-art performance in many settings. Understanding the interplay between `n_estimators`, `max_features`, and the depth/complexity of individual trees is key to optimizing performance.\n",
      "\n",
      "In essence, Random Forest builds a \"wisdom of the crowd\" by training diverse, yet powerful, individual learners and aggregating their opinions. This combination of randomness and aggregation makes it an exceptionally robust and effective machine learning model.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a senior machine learning engineer with 15 years of experience.\n",
    "You explain concepts using technical depth and mathematical intuition.\n",
    "\n",
    "Explain {topic}.\n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "print(chain.invoke({'topic':'Random Forest'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57a203a-8b09-4947-b42a-9e6e3c5afdd5",
   "metadata": {},
   "source": [
    "## Chain of Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86549dfa-1967-4356-9f16-1d7cc0170084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the system of equations:\n",
      "1) x - y + 1 = 0\n",
      "2) x + y - 1 = 0\n",
      "\n",
      "We can use the elimination method. Notice that the 'y' terms have opposite signs. If we add the two equations together, the 'y' terms will cancel out.\n",
      "\n",
      "Step 1: Add Equation 1 and Equation 2.\n",
      "(x - y + 1) + (x + y - 1) = 0 + 0\n",
      "Combine like terms:\n",
      "(x + x) + (-y + y) + (1 - 1) = 0\n",
      "2x + 0 + 0 = 0\n",
      "2x = 0\n",
      "\n",
      "Step 2: Solve for x.\n",
      "2x = 0\n",
      "x = 0 / 2\n",
      "x = 0\n",
      "\n",
      "Step 3: Substitute the value of x (x=0) into either of the original equations to solve for y. Let's use Equation 2:\n",
      "x + y - 1 = 0\n",
      "Substitute x = 0:\n",
      "0 + y - 1 = 0\n",
      "y - 1 = 0\n",
      "y = 1\n",
      "\n",
      "Step 4: Verify the solution by substituting x=0 and y=1 into both original equations.\n",
      "For Equation 1: x - y + 1 = 0\n",
      "0 - 1 + 1 = 0\n",
      "0 = 0 (True)\n",
      "\n",
      "For Equation 2: x + y - 1 = 0\n",
      "0 + 1 - 1 = 0\n",
      "0 = 0 (True)\n",
      "\n",
      "Both equations are satisfied, so the solution is correct.\n",
      "\n",
      "The final answer is $\\boxed{x=0, y=1}$.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Solve step by step:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Reason carefully before giving final answer.\n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "print(chain.invoke({'question':'solve x-y+1 = 0 and x+y-1 = 0'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a446ca93-eadb-436a-acc7-10c509632578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
